<template>
  <div class="home">
    <el-collapse v-model="activeName" accordion>
      <el-collapse-item title="abstract" name="0">
        <img src="../assets/0.png" alt="" srcset="" />
        <p>
          Our project takes a portrait image and a target lighting as input and
          generates a new portrait image.
        </p>
      </el-collapse-item>
      <el-collapse-item title="1. PaddleSeg" name="1">
        <img src="../assets/1-1.png" alt="" srcset="" />
        <p>The Overview of PaddleSeg Toolkit.</p>
        <img src="../assets/1-2.png" alt="" srcset="" />
        <p>The segmentation network</p>
        <img src="../assets/1-3.png" alt="" srcset="" />
        <p>The Encoder-Decoder</p>
      </el-collapse-item>
      <el-collapse-item title="2. Dense Passage Retriever (DPR)" name="2">
        <img src="../assets/2-1.png" alt="" srcset="" />
        <p>
          DPR implemented using dense representations alone, without the need
          for traditional sparse vector space models like TF-IDF or BM25.
        </p>
        <img src="../assets/2-2.png" alt="" srcset="" />
        <p>Comparison of training methods.</p>
      </el-collapse-item>
      <el-collapse-item title="3. spherical harmonics" name="3">
        <img src="../assets/3-1.png" alt="" srcset="" />
        <p>三阶SH模拟</p>
        <p>
          Diffuse IBL The original ambient light data is directly stored into a
          three-order SH coefficient, and the pre-stored coefficient is directly
          used for SH projection during calculation
        </p>
        <img src="../assets/3-2.png" alt="" srcset="" />
        <p>
          If you output the function and the original function of the projection
          formula to the Cartesian xyz space, you get the following image. The
          higher the value of l, the more accurate the restoration of the
          original function:
        </p>
      </el-collapse-item>
      <el-collapse-item title="4. ARAP-Based Normal Refinement" name="4">
        <img src="../assets/4-1.png" alt="" srcset="" />
        <p>
          The estimated warp function by ARAP is then applied to the face
          normals estimated by 3DDFA to get refined normals as illustrated in
          this Figure.
        </p>
      </el-collapse-item>
      <el-collapse-item
        title="5. Main Architecture for Portrait Relighting"
        name="5"
      >
        <img src="../assets/5-1.png" alt="" srcset="" />
        <p>The structure of the proposed Hourglass network.</p>
        <div>
          We thus propose a skip training strategy in which we train our network
          without skip connections first, then add skip layers one by one during
          subsequent training. We denote this as skip training. Figure 6
          compares the relit images generated by removing the skip layers of
          vanilla Hourglass network and Hourglass network with skip training.
        </div>
        <img src="../assets/5-2.png" alt="" srcset="" />
        <p>
          Skip training can help improve the quality of the generated results by
          removing artifacts around the nose.
        </p>
        <img src="../assets/5-3.png" alt="" srcset="" />
      </el-collapse-item>
      <el-collapse-item title="6.result" name="6">
        <img src="../assets/6-1.png" alt="" srcset="" />
      </el-collapse-item>
    </el-collapse>
  </div>
  <div
    ref="previewContainer"
    class="preview-container"
    v-show="showPreview"
    @click="closePreview"
  >
    <img :src="imageSrc" class="preview" alt="Preview Image" />
  </div>
</template>

<script setup>
import { onMounted, ref } from 'vue'
const activeName = ref('0')
const imageSrc = ref(null)
const showPreview = ref(false)
const preview = (e) => {
  console.log('放大')
  imageSrc.value = e.target.src
  openPreview()
}
const openPreview = () => {
  showPreview.value = true
}
const closePreview = () => {
  showPreview.value = false
}
onMounted(() => {
  document.querySelectorAll('img:not(.preview)').forEach((img) => {
    img.addEventListener('click', preview)
  })
})
</script>

<style lang="less" scoped>
.home {
  width: 60%;
  margin: 0 auto;
  .el-collapse {
    background-color: transparent;
    border-color: transparent;
    --el-collapse-content-bg-color: transparent;
    --el-collapse-header-bg-color: transparent;
    --el-collapse-border-color: transparent;
    --el-collapse-header-font-size: 25px;
    --el-collapse-content-font-size: 20px;
    --el-collapse-content-text-color: #fff;
    --el-collapse-header-text-color: #fff;
    .el-collapse-item {
      img {
        width: 90%;
        display: block;
        margin: 0 auto;
        cursor: pointer;
      }
      p + img {
        margin-top: 20px;
      }
    }
  }
}

.preview-container {
  position: fixed;
  top: 50%;
  left: 50%;
  transform: translate(-50%, -50%);
  z-index: 9999;
  background-color: rgba(0, 0, 0, 0.8);
  padding: 20px;
  width: 100vw;
  height: 100vh;
  img {
    object-fit: contain;
    width: 100%;
    height: 100%;
    cursor: pointer;
  }
}
</style>
